{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n",
    "# Qwen/Qwen3-4B\n",
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('rollouts.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy_by_source = {}\n",
    "for line in data:\n",
    "    if line['data_source'] not in accuracy_by_source:\n",
    "        if 'bbh_' in line['data_source']:\n",
    "            line['data_source'] = 'bbh'\n",
    "        accuracy_by_source[line['data_source']] = []\n",
    "    accuracy_by_source[line['data_source']].append(line['final_reward'])\n",
    "    \n",
    "for source in accuracy_by_source:\n",
    "    mean_accuracy = np.mean(accuracy_by_source[source])\n",
    "    print(source, mean_accuracy, len(accuracy_by_source[source]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c78bcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math_verify.errors import TimeoutException\n",
    "from math_verify.metric import math_metric\n",
    "from math_verify.parser import ExprExtractionConfig, LatexExtractionConfig\n",
    "\n",
    "def clean_prediction(model_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to normalize model outputs like 'Answer: C. Cotton' into '\\boxed{C}'.\n",
    "    \"\"\"\n",
    "\n",
    "    # If boxed answer is already there, keep it\n",
    "    boxed_match = re.search(r'\\\\boxed\\{([A-Za-z0-9]+)\\}', model_output)\n",
    "    if boxed_match:\n",
    "        return f\"\\\\boxed{{{boxed_match.group(1)}}}\"\n",
    "\n",
    "    # Otherwise, try to find a single-letter multiple-choice answer\n",
    "    choice_match = re.search(r'\\b([A-E])\\b', model_output)\n",
    "    if choice_match:\n",
    "        return f\"\\\\boxed{{{choice_match.group(1)}}}\"\n",
    "\n",
    "    # If nothing found, just return as-is\n",
    "    return model_output.strip()\n",
    "\n",
    "def getRawCorrectness(model_output: str, ground_truth: str) -> float:\n",
    "    verify_func = math_metric(\n",
    "        gold_extraction_target=(LatexExtractionConfig(),),\n",
    "        pred_extraction_target=(ExprExtractionConfig(), LatexExtractionConfig()),\n",
    "    )\n",
    "\n",
    "    ret_score = 0.0\n",
    "    ground_truth_boxed = f\"\\\\boxed{{{ground_truth}}}\"\n",
    "\n",
    "    try:\n",
    "        pred_clean = clean_prediction(model_output)\n",
    "        ret_score, _ = verify_func([ground_truth_boxed], [pred_clean])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return ret_score\n",
    "\n",
    "def getCorrectness(model_output: str, ground_truth: str) -> float:\n",
    "    if '</think>' in model_output:\n",
    "        model_output = model_output.split('</think>')[-1]\n",
    "\n",
    "    verify_func = math_metric(\n",
    "        gold_extraction_target=(LatexExtractionConfig(),),\n",
    "        pred_extraction_target=(ExprExtractionConfig(), LatexExtractionConfig()),\n",
    "    )\n",
    "\n",
    "    ret_score = 0.0\n",
    "    ground_truth_boxed = f\"\\\\boxed{{{ground_truth}}}\"\n",
    "\n",
    "    try:\n",
    "        pred_clean = clean_prediction(model_output)\n",
    "        ret_score, _ = verify_func([ground_truth_boxed], [pred_clean])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return ret_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "correctness_response_length_list = {}\n",
    "from tqdm import tqdm\n",
    "for line in tqdm(data):\n",
    "    if line['data_source'] not in correctness_response_length_list:\n",
    "        correctness_response_length_list[line['data_source']] = []\n",
    "    question = line['prompt']\n",
    "    correctness_response_length_list[line['data_source']].append((question, getCorrectness(line['response'], line['ground_truth']), len(tokenizer.encode(line['response']))))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22c5215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aucoaa(data):\n",
    "    # data is a list of (correctness, response_length) tuples\n",
    "    n = len(data)\n",
    "    tmax = max(length for _,_, length in data)\n",
    "    oaa_t_list = []\n",
    "    for t in range(tmax + 1):\n",
    "        filtered = [correct for _,correct, length in data if length < t]\n",
    "        if filtered:\n",
    "            oaa_t = sum(filtered) / n  # Denominator is always n, per definition\n",
    "        else:\n",
    "            oaa_t = 0.0\n",
    "        oaa_t_list.append(oaa_t)\n",
    "    aucoaa = sum(oaa_t_list) / len(oaa_t_list)\n",
    "    return aucoaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08506db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_accuracy_from_runs(prompts, accuracies, n_runs=5):\n",
    "    \"\"\"\n",
    "    prompts: list of prompt IDs (each repeated n_runs times)\n",
    "    accuracies: list of 0/1, same length as prompts\n",
    "    n_runs: number of answers per prompt (default 5)\n",
    "    \n",
    "    Returns:\n",
    "        dict with per-answer, per-prompt, and per-run accuracy + std\n",
    "    \"\"\"\n",
    "    assert len(prompts) == len(accuracies), \"Both lists must have the same length\"\n",
    "\n",
    "    accuracies = np.array(accuracies)\n",
    "    total_answers = len(accuracies)\n",
    "\n",
    "    # ------------------\n",
    "    # Per-answer accuracy (binomial estimate)\n",
    "    # ------------------\n",
    "    acc_answer = accuracies.mean()\n",
    "    std_answer = np.sqrt(acc_answer * (1 - acc_answer) / total_answers)\n",
    "\n",
    "    # ------------------\n",
    "    # Per-prompt accuracy\n",
    "    # ------------------\n",
    "    prompt_correct = defaultdict(list)\n",
    "    for p, a in zip(prompts, accuracies):\n",
    "        prompt_correct[p].append(a)\n",
    "\n",
    "    prompt_solved = np.array([np.mean(v) for v in prompt_correct.values()])\n",
    "    acc_prompt = prompt_solved.mean()\n",
    "    std_prompt = prompt_solved.std(ddof=1)\n",
    "\n",
    "    # ------------------\n",
    "    # Per-run accuracy (like papers report)\n",
    "    # ------------------\n",
    "    # initialize n_runs lists (one per run)\n",
    "    run_correct = [[] for _ in range(n_runs)]\n",
    "\n",
    "    for p in set(prompts):\n",
    "        answers = prompt_correct[p]   # length = n_runs\n",
    "        for i, a in enumerate(answers):\n",
    "            run_correct[i].append(a)\n",
    "\n",
    "    run_accs = np.array([np.mean(r) for r in run_correct])\n",
    "    acc_run = run_accs.mean()\n",
    "    std_run = run_accs.std(ddof=1)\n",
    "\n",
    "    return {\n",
    "        \"per_run_accuracy\": acc_run,\n",
    "        \"per_run_std\": std_run,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in correctness_response_length_list.keys():\n",
    "    print(\"Data Source: \", key)\n",
    "    prompt_list = [prompt for prompt, _, _ in correctness_response_length_list[key]]\n",
    "    accuracy_list = [correct for _, correct, _ in correctness_response_length_list[key]]\n",
    "    response_length_list = [length for _, _, length in correctness_response_length_list[key]]\n",
    "\n",
    "    result = compute_accuracy_from_runs(prompt_list, accuracy_list)\n",
    "    print(f\"Accuracy: {result['per_run_accuracy']}\")\n",
    "    print(f\"Std: {result['per_run_std']}\")\n",
    "    print(f\"Response Length: {np.mean(response_length_list)}\")\n",
    "    print(f\"Max Response Length: {max(response_length_list)}\")\n",
    "    print(f\"Min Response Length: {min(response_length_list)}\")\n",
    "\n",
    "    print(\"AUC-OAA: \", compute_aucoaa(correctness_response_length_list[key]))\n",
    "    print(\"--------------------------------\")\n",
    "    \n",
    "    # print(result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
