{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "# Qwen/Qwen3-4B\n",
    "# deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('underthink_rollout.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34625e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_boxed_only_string(string):\n",
    "    idx = string.rfind(\"\\\\boxed\")\n",
    "    if idx < 0:\n",
    "        idx = string.rfind(\"\\\\fbox\")\n",
    "        if idx < 0:\n",
    "            return None\n",
    "\n",
    "    i = idx\n",
    "    right_brace_idx = None\n",
    "    num_left_braces_open = 0\n",
    "    while i < len(string):\n",
    "        if string[i] == \"{\":\n",
    "            num_left_braces_open += 1\n",
    "        if string[i] == \"}\":\n",
    "            num_left_braces_open -= 1\n",
    "            if num_left_braces_open == 0:\n",
    "                right_brace_idx = i\n",
    "                break\n",
    "        i += 1\n",
    "    if right_brace_idx is None:\n",
    "        retval = None\n",
    "    else:\n",
    "        retval = string[idx : right_brace_idx + 1]\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "def remove_boxed(s):\n",
    "    left = \"\\\\boxed{\"\n",
    "    try:\n",
    "        assert s[: len(left)] == left\n",
    "        assert s[-1] == \"}\"\n",
    "        return s[len(left) : -1]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_boxed_answer(solution: str) -> str:\n",
    "    \"\"\"Extract the answer from inside a LaTeX \\\\boxed{} command\"\"\"\n",
    "    solution = last_boxed_only_string(solution)\n",
    "    solution = remove_boxed(solution)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffef363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('scripts/data/optimalThinkingBench/underthink_bench.json', 'r') as f:\n",
    "    reference_dataset = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a5d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reasoning_gym\n",
    "from reasoning_gym.factory import create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d78d9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzles = {\n",
    "    \"ab\": {\"length\": 20},\n",
    "    \"advanced_geometry\": {\"min_coord\": -20, \"max_coord\": 20},\n",
    "    \"knight_swap\": {},\n",
    "    \"tsumego\": {\"min_board_size\": 15, \"max_board_size\": 19, \"max_stones\": 20},\n",
    "    \"fraction_simplification\": {},\n",
    "    \"propositional_logic\": {},\n",
    "    \"bitwise_arithmetic\": {\"difficulty\": 4},\n",
    "    \"letter_counting\": {\"min_words\": 25, \"max_words\": 35},\n",
    "    \"maze\": {\"min_dist\": 15, \"max_dist\": 25, \"min_grid_size\": 15, \"max_grid_size\": 25},\n",
    "    \"puzzle24\": {\"min_value\": 8, \"max_value\": 10},\n",
    "    \"quantum_lock\": {\"difficulty\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53007cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_response_length_list = {}\n",
    "from tqdm import tqdm\n",
    "for d in tqdm(data):\n",
    "    generated_answer = extract_boxed_answer(d['response'])\n",
    "    for i in range(len(reference_dataset)):\n",
    "        if reference_dataset[i]['uuid'] == d['uuid']:\n",
    "            gold_entry = reference_dataset[i]\n",
    "            puzzle = gold_entry['puzzle']\n",
    "            metadata = gold_entry['metadata']\n",
    "            dataset = reasoning_gym.create_dataset(puzzle, size=50, **puzzles[puzzle])\n",
    "            break\n",
    "    if gold_entry is not None:\n",
    "        score = dataset.score_answer(answer=generated_answer, entry=gold_entry)\n",
    "        if score < 1:\n",
    "            score = 0\n",
    "    if d['data_source'] not in correctness_response_length_list:\n",
    "        correctness_response_length_list[d['data_source']] = []\n",
    "    correctness_response_length_list[d['data_source']].append((score, len(tokenizer.encode(d['response']))))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "450d4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aucoaa(data):\n",
    "    # data is a list of (correctness, response_length) tuples\n",
    "    n = len(data)\n",
    "    tmax = max(length for _, length in data)\n",
    "    oaa_t_list = []\n",
    "    for t in range(tmax + 1):\n",
    "        filtered = [correct for correct, length in data if length < t]\n",
    "        if filtered:\n",
    "            oaa_t = sum(filtered) / n  # Denominator is always n, per definition\n",
    "        else:\n",
    "            oaa_t = 0.0\n",
    "        oaa_t_list.append(oaa_t)\n",
    "    aucoaa = sum(oaa_t_list) / len(oaa_t_list)\n",
    "    return aucoaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for key in correctness_response_length_list.keys():\n",
    "    mean_response_length = np.mean([length for _, length in correctness_response_length_list[key]])\n",
    "    accuracy = np.mean([correct for correct, _ in correctness_response_length_list[key]])\n",
    "    print(\"Data Source: \", key)\n",
    "    print(\"Mean Response Length: \", mean_response_length)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Number of Samples: \", len(correctness_response_length_list[key]))\n",
    "    print(\"AUC-OAA: \", compute_aucoaa(correctness_response_length_list[key]))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f361d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
